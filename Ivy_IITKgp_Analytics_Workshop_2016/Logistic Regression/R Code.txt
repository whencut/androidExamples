# This is how you call the packages
library(caret)
library(ggplot2)
library(MASS)
library(car)
library(mlogit)


setwd("C:\\Users\\Subhojit\\Desktop\\IIT KGP\\Logistic Regression")

# Reading data; just change the file path to fetch the data.
data <- read.csv("Analysis_of_Default.csv",header = TRUE,na.strings="")

# Data sanity check
dim(data)
str(data)

# Converting necessary variables into factor
#data$Default_On_Payment <- as.factor(data$Default_On_Payment)



# Logistic Regression on full data

model <- glm(Default_On_Payment~., data=data, family=binomial())
summary(model)

## Remove the insignificant variable
model <- glm(Default_On_Payment~ Attr1_Dummy1 +	Attr1_Dummy2 +	Attr1_Dummy3 +	Attr2_Trans +	Attr3_dummy1 
+	Attr3_dummy2 +	Attr3_dummy4 +	Attr4_Dummy1 +	Attr4_Dummy2 +	Attr4_Dummy3 +	Attr4_Dummy6 
+	Attr5_Dummy1 +	Attr5_Dummy2 +	Attr6_Dummy1 +	Attr6_Dummy3 +	Attr6_Dummy4 +	Attr7_Dummy3 +	Attr8 
+	Attr9_Dummy1 +	Attr9_Dummy2 +	Attr9_Dummy3 +	Attr10_Dummy2 +	Attr12_Dummy1 +	Attr13 +	Attr14_Dummy 
+	Attr15_Dummy +	Attr16 +	Attr19_Dummy +	Attr20
, data=data, family=binomial())
summary(model)

## Remove the insignificant variable
model <- glm(Default_On_Payment~ Attr1_Dummy1 +	Attr1_Dummy2 +	Attr1_Dummy3 +	Attr2_Trans +	Attr3_dummy1 
+	Attr3_dummy2 +	Attr3_dummy4 +	Attr4_Dummy1 +	Attr4_Dummy2 +	Attr4_Dummy3 +	Attr4_Dummy6 
+	Attr5_Dummy1 +	Attr5_Dummy2 +	Attr6_Dummy3 +	Attr6_Dummy4 +	Attr7_Dummy3 +	Attr8 
+	Attr9_Dummy1 +	Attr9_Dummy2 +	Attr9_Dummy3 +	Attr10_Dummy2 +	Attr12_Dummy1 +	Attr13 +	Attr14_Dummy 
+	Attr15_Dummy +	Attr16 +	Attr19_Dummy +	Attr20
, data=data, family=binomial())
summary(model)

vif(model)

# Deviance is -2*Log Likelyhood
# AIC = -2LL + 2k
# BIC = -2LL + 2k x log(n)





# Difference between -2LL of Null model and model with variables
modelChi <- model$null.deviance - model$deviance
modelChi

#Finding the degree of freedom for Null model and model with variables
chidf <- model$df.null - model$df.residual
chidf

# With more decimal places
# If p value is less than .05 then we reject the null hypothesis that the model is no better than chance.
chisq.prob <- 1 - pchisq(modelChi, chidf)
format(round(chisq.prob, 2), nsmall = 5)


# Hosmer and Lemeshow R square
R2.hl<-modelChi/model$null.deviance
R2.hl


# Cox and Snell R Square (the last number; here is 5000 should be total no. of ovservation)

R.cs <- 1 - exp ((model$deviance - model$null.deviance) /3000)
R.cs

# Max rescaled R square (Nagelkarke) (the last number; here is 5000 should be total no. of ovservation)

R.n <- R.cs /(1-(exp(-(model$null.deviance/3000))))
R.n




#######################################################################################
#Function - HS Test

hosmerlem <- function (y, yhat, g = 10) {
   cutyhat <- cut(yhat, breaks = quantile(yhat, probs = seq(0, 1, 1/g)),
                  include.lowest = TRUE)
   obs <- xtabs(cbind(1 - y, y) ~ cutyhat)
   expect <- xtabs(cbind(1 - yhat, yhat) ~ cutyhat)
   chisq <- sum((obs - expect)^2 / expect)
   P <- 1 - pchisq(chisq, g - 2)
   c("X^2" = chisq, Df = g - 2, "P(>Chi)" = P)
 }
################################################################################################################
# How to use the function. Data.train is the name of the dataset. model is name of the glm output
## This is not a very useful test. Some authors have suggested that sometimes it produces wrong result##

hosmerlem(y = as.integer(data$Default_On_Payment), yhat = fitted(model))
################################################################################################################
# Hosmer and Lemeshow test in a different way

library(ResourceSelection)
hl <- hoslem.test(as.integer(data$Default_On_Payment), fitted(model), g=10)
hl

#####################################################################################################################
# Coefficients (Odds)
model$coefficients
# Coefficients (Odds Ratio)
exp(model$coefficients)

# Predicted Probabilities
prediction <- predict(model,newdata = data,type="response")
prediction

#write.csv(prediction, file = "C:\\Users\\Subhojit\\Desktop\\Logistic Regression\\Prepared by me\\pred.csv")

library(pROC)
rocCurve   <- roc(response = data$Default_On_Payment, predictor = prediction, levels = rev(levels(data$Default_On_Payment)))

#Metrics - Fit Statistics

predclass <-ifelse(prediction>coords(rocCurve,"best")[1],1,0)
Confusion <- table(Predicted = predclass,Actual = data$Default_On_Payment)
AccuracyRate <- sum(diag(Confusion))/sum(Confusion)
Gini <-2*auc(rocCurve)-1

AUCmetric <- data.frame(c(coords(rocCurve,"best"),AUC=auc(rocCurve),AccuracyRate=AccuracyRate,Gini=Gini))
AUCmetric <- data.frame(rownames(AUCmetric),AUCmetric)
rownames(AUCmetric) <-NULL
names(AUCmetric) <- c("Metric","Values")
AUCmetric

Confusion 
plot(rocCurve)

#########################################################################################################################
### KS statistics calculation
data$m1.yhat <- predict(model, data, type = "response")

library(ROCR)
m1.scores <- prediction(data$m1.yhat, data$Default_On_Payment)

plot(performance(m1.scores, "tpr", "fpr"), col = "red")
abline(0,1, lty = 8, col = "grey")

m1.perf <- performance(m1.scores, "tpr", "fpr")
ks1.logit <- max(attr(m1.perf, "y.values")[[1]] - (attr(m1.perf, "x.values")[[1]]))
ks1.logit # Thumb rule : should lie between 40 - 70

############################################################################################################



### skipping it since it is not used in the industry ##########
###################### Residual Analysis ################################################################################


logistic_data <- data.test

logistic_data$predicted.probabilities<-fitted(modelt)
logistic_data$standardized.residuals<-rstandard(modelt)
logistic_data$studentized.residuals<-rstudent(modelt)
logistic_data$dfbeta<-dfbeta(modelt)
logistic_data$dffit<-dffits(modelt)
logistic_data$leverage<-hatvalues(modelt)

#logistic_data[, c("leverage", "studentized.residuals", "dfbeta")]
#write.csv(logistic_data, file = "C:\\Users\\Subhojit\\Desktop\\Logistic Regression\\Prepared by me\\pred.csv")

