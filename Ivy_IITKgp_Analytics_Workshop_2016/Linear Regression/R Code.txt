library(boot) 
library(car)
library(QuantPsyc)
library(lmtest)
library(sandwich)
library(vars)
library(nortest)
library(MASS)


#Set Working Directory
setwd("C:\\Users\\Subhojit\\Desktop\\IIT KGP\\Linear Regression")

#Read the data file
data<-read.csv("new_data.csv")

#Check if the data is populated/imported properly
head(data)
tail(data)

#Check the summary of the file
summary(data)


#Convert Categorical varibles in Dummy Variable
data$GenderDummy<-ifelse(data$Gender=="F",1,0)

# dropping the original Gender varaible
data<- data[,-c(6)]

#Similarly we can covert other categorical variables to Dummy Variables
data$fuel<-ifelse(data$Fuel.Type=="D",1,0)
data$married<-ifelse(data$Married=="Married",1,0)


# dropping the original  varaible: Married and Fuel.Type
data<- data[,-c(6,8)]

head(data)






#Create linear function for 
# run the regression

fit<-lm(Losses~Policy.Number+	Years.of.Driving.Experience	+Number.of.Vehicles    +Vehicle.Age	  +Age
+GenderDummy	+married	+fuel +Tag ,data=data)

summary(fit)

## remove Tag and Policy Number
fit<-lm(Losses~	Years.of.Driving.Experience	+Number.of.Vehicles    +Vehicle.Age	  +Age
+GenderDummy	+married	+fuel  ,data=data)

summary(fit)

## remove Number of Vehecles
fit<-lm(Losses~	Years.of.Driving.Experience    +Vehicle.Age	  +Age
+GenderDummy	+married	+fuel  ,data=data)

summary(fit)



## Final model...
fit<-lm(Losses~	Years.of.Driving.Experience    +Vehicle.Age	  +Age
+GenderDummy	+married	+fuel  ,data=data)

summary(fit)

#Check Vif, vif>2 means presence of multicollinearity
vif(fit)

## Final model...
fit<-lm(Losses~	Vehicle.Age	  +Age +GenderDummy	+married	+fuel  ,data=data)

summary(fit)

#Check Vif, vif>2 means presence of multicollinearity
vif(fit)

## Get the predicted or fitted values
fitted(fit)

## MAPE
data$pred <- fitted(fit)
#write.csv(ori_data,"mape.csv")

#Calculating MAPE
attach(data)
(sum((abs(Losses-pred))/Losses))/nrow(data)




### skip this part since it is rarely used in the industry ###############

############ Residual Analysis ############################################################################

res <- ori_data

res$stu_res <- studres(fit) ##Studentized residuals
res$stud.del.resids <- rstudent(fit) ##studentized deleted residuals
res$leverage <- hatvalues(fit) ## leverage values (hi)
res$cooks_dis <- cooks.distance(fit) ## Cook's distance
res$dffits <- dffits(fit) ## Dffit
res$dfbetas <- dfbetas(fit) ## Dfbetas
res$cov_ratio <- covratio(fit) ## Covariance Ratio

#write.csv(res,"res.csv")


##################################### Checking of Assumption ############################################

# residuals should be uncorrelated ##Autocorrelation
# Null H0: residuals from a linear regression are uncorrelated. Value should be close to 2. 
#Less than 1 and greater than 3 -> concern

durbinWatsonTest(fit)
dwt(fit)

# Checking multicollinearity
vif(fit) # should be within 2. If it is greater than 10 then serious problem


################ Constant error variance ##########Heteroscedasticity


# Breusch-Pagan test
bptest(fit)  # Null hypothesis -> error is homogenious (p value should be more than 0.05)

#Cook-Weisberg test
# hypothesis of constant error variance against the alternative that the error variance 
#changes with the level of the  response 
# p value should be more than 0.05
ncvTest(lm(Losses~	Vehicle.Age	  +Age +GenderDummy	+married	+fuel  ,data=ori_data))



## Normality testing Null hypothesis is data is normal.

resids <- fit$residuals


ad.test(resids) #get Anderson-Darling test for normality 
cvm.test(resids) #get Cramer-von Mises test for normaility 
lillie.test(resids) #get Lilliefors (Kolmogorov-Smirnov) test for normality 
pearson.test(resids) #get Pearson chi-square test for normaility 
sf.test(resids) #get Shapiro-Francia test for normaility 

qqnorm(resids)

#su <-  rnorm(5000, mean = 0, sd = 1)
#qqnorm(su)



